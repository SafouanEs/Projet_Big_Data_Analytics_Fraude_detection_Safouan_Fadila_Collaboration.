{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Projet**: **Big** **Data** **Analytics**.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "                     Detéction de Faude dans les transactions Financières.\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "**Réalisé par:**\n",
        "\n",
        "*   Fadila Alaoui.(IIN)\n",
        "*   Safouan Garaaouch.(ICSD)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T9oSTo9jQ_3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   **Contexte du Projet:**\n",
        "\n",
        "Le secteur financier est confronté à un défi croissant en matière de fraude dans les transactions. Les fraudes financières peuvent avoir un impact significatif sur les institutions financières ainsi que sur les clients. Dans ce contexte, le projet de Big Data Analytics vise à développer un système robuste de détection de fraude basé sur l'analyse avancée de données provenant de transactions financières en utlisant Pysaprk et MLlib.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DHgv1JggSbjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*   **Descreption** **du jeu** **de** **donnée:**\n",
        "\n",
        "\n",
        "> **Objectif**:\n",
        "\n",
        " le jeu de données utlisé dans notre projet est une collection synthétique de transactions financières,créée dans le but de détecter la fraude. générés à l'aide de de l'IA générative pour produire des données réalistes mais non réelles qui vont être utilisées pour former des modèles d'apprentissage automatique sans les préoccupations de confidentialité et de sécurité liées à l'utilisation de données clients réelles.\n",
        "\n",
        "\n",
        "> **Dictionnaire du jeu de données:**\n",
        "\n",
        "Le jeu de données contient des informations sur des transactions financières avec les colonnes suivantes :\n",
        "\n",
        "**Transaction_ID :** Un identifiant unique pour chaque transaction.\n",
        "\n",
        "**Date :** La date et l'heure auxquelles la transaction a eu lieu.\n",
        "\n",
        "**Montant :** Le montant d'argent impliqué dans la transaction.\n",
        "\n",
        "**Commerçant :** Le type de commerçant où la transaction a eu lieu (par exemple, Restaurant, Supermarché).\n",
        "\n",
        "**Emplacement :** La localisation géographique de la transaction (par exemple, Phoenix, New York).\n",
        "\n",
        "**User_ID :** Un identifiant pour l'utilisateur qui a effectué la transaction.\n",
        "\n",
        "**Frauduleux :** Un indicateur signalant si la transaction était frauduleuse (0 pour non, 1 pour oui).\n",
        "\n",
        "**Type_Transaction :** La nature de la transaction (par exemple, Dépôt, Paiement, Retrait).\n",
        "\n",
        "**Mode_Saisie :** Comment la transaction a été saisie (par exemple, Puce, En ligne, Saisie manuelle).\n",
        "\n",
        "Age_Compte : L'âge du compte en années au moment de la transaction.\n",
        "\n",
        "**Transactions_Précédentes :** Le nombre de transactions précédentes que l'utilisateur avait effectuées.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K66IQeZ_Tmuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "> Installation de Pyspark dans notre environnement PySpark qui fournit une API Python conviviale pour interagir avec les fonctionnalités pour traiter les données. En utlisant Pip\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ObS0qWqWZ8DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5fMgTxzJKex",
        "outputId": "44f35658-6330-4302-a611-4fc18420930e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=6488abf21dfeee4c0a0959f1deb10e4fd6a0075765244b6e423727183e29aba3\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importation des bibliothèques nécessaires:\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
        "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, GBTClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.mllib.stat import Statistics\n",
        "from pyspark.sql.functions import isnan, when, count, col\n",
        "from pyspark.sql.types import DoubleType, FloatType\n",
        "from pyspark.sql.functions import col, to_date, date_format\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "import random\n",
        "from pyspark.sql.functions import year, month, dayofmonth, dayofweek, hour\n",
        "from pyspark.ml.feature import StandardScaler\n"
      ],
      "metadata": {
        "id": "IFgi85v-JIQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"Fraud Detection\").getOrCreate()"
      ],
      "metadata": {
        "id": "ZnKYccb-Jkxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "df = spark.read.csv('/content/corrected_enriched_financial_transactions_30k.csv', header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "pFmLJNfKJw4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 1.   Analyse Exploratoire des données:\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YFyXCHQOJ4rG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Afficher les cinq premières lignes du DataFrame.\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_XJyNt9LGIy",
        "outputId": "d502173c-b11e-42ff-c847-355007eaca62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------------+------------------+-----------+--------+-------+----------+----------------+------------+------------------+---------------------+\n",
            "|Transaction_ID|                Date|            Amount|   Merchant|Location|User_ID|Fraudulent|Transaction_Type|  Entry_Mode|       Account_Age|Previous_Transactions|\n",
            "+--------------+--------------------+------------------+-----------+--------+-------+----------+----------------+------------+------------------+---------------------+\n",
            "|           447|2023-11-27 15:00:...|1029.1933632539153| Restaurant| Phoenix|   1788|         0|         Deposit|        Chip|3.5163073310123765|   16.933340293078178|\n",
            "|           683|2023-09-28 15:00:...| 228.9851910465608|Supermarket|New York|   1862|         0|         Payment|        Chip| 5.572630448789214|    9.596356135771336|\n",
            "|           804|2023-08-16 15:00:...| 909.8757369368176| Restaurant|New York|   1390|         0|      Withdrawal|        Chip| 3.761729866948193|   11.991090296282167|\n",
            "|           146|2023-04-07 15:00:...|2920.8965658000293|   Clothing| Houston|   1483|         0|      Withdrawal|      Online| 4.024156332926795|      6.7725526867406|\n",
            "|           712|2023-03-20 15:00:...| 613.4804158931692|Electronics|New York|   1633|         0|      Withdrawal|Manual Entry| 5.164395229440814|    7.388250962423852|\n",
            "+--------------+--------------------+------------------+-----------+--------+-------+----------+----------------+------------+------------------+---------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#afficher le schéma d'un DataFrame.\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0MtV4F0P_vo",
        "outputId": "ddcda41b-3cb3-424c-f299-c29ba95ef1d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Transaction_ID: integer (nullable = true)\n",
            " |-- Date: timestamp (nullable = true)\n",
            " |-- Amount: double (nullable = true)\n",
            " |-- Merchant: string (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- User_ID: integer (nullable = true)\n",
            " |-- Fraudulent: integer (nullable = true)\n",
            " |-- Transaction_Type: string (nullable = true)\n",
            " |-- Entry_Mode: string (nullable = true)\n",
            " |-- Account_Age: double (nullable = true)\n",
            " |-- Previous_Transactions: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#générer un résumé statistique des colonnes  du DataFrame.\n",
        "df.describe().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16NOlZyUQI7Q",
        "outputId": "caf46214-3262-4f26-a590-9d2ee4b3dc87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+------------------+-----------+--------+------------------+------------------+----------------+----------+------------------+---------------------+\n",
            "|summary|    Transaction_ID|            Amount|   Merchant|Location|           User_ID|        Fraudulent|Transaction_Type|Entry_Mode|       Account_Age|Previous_Transactions|\n",
            "+-------+------------------+------------------+-----------+--------+------------------+------------------+----------------+----------+------------------+---------------------+\n",
            "|  count|             30000|             30000|      30000|   30000|             30000|             30000|           30000|     30000|             30000|                30000|\n",
            "|   mean| 514.3299333333333|2515.0564320899994|       NULL|    NULL|         1486.5497|            0.0322|            NULL|      NULL|4.5091770873363535|   10.031656883173788|\n",
            "| stddev|296.81802501756613|1454.5520169491997|       NULL|    NULL|285.64160673968127|0.1765338460653573|            NULL|      NULL|2.0896771061202064|   3.6444393134226623|\n",
            "|    min|                 1| 45.49177030168343|   Clothing| Chicago|              1000|                 0|         Deposit|      Chip|               0.0|                  0.0|\n",
            "|    max|              1028| 5486.503441103155|Supermarket| Phoenix|              1999|                 1|      Withdrawal|    Online| 14.35653067329035|   25.856963898361112|\n",
            "+-------+------------------+------------------+-----------+--------+------------------+------------------+----------------+----------+------------------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Code ajusté pour gérer correctement les colonnes non numériques\n",
        "df.select([\n",
        "    count(when(col(c).isNull(), c)).alias(c) if df.schema[c].dataType not in [DoubleType(), FloatType()]\n",
        "    else count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns\n",
        "]).show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy01eentQPkX",
        "outputId": "d6e528a2-62eb-449b-b16b-46188aeded7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+----+------+--------+--------+-------+----------+----------------+----------+-----------+---------------------+\n",
            "|Transaction_ID|Date|Amount|Merchant|Location|User_ID|Fraudulent|Transaction_Type|Entry_Mode|Account_Age|Previous_Transactions|\n",
            "+--------------+----+------+--------+--------+-------+----------+----------------+----------+-----------+---------------------+\n",
            "|             0|   0|     0|       0|       0|      0|         0|               0|         0|          0|                    0|\n",
            "+--------------+----+------+--------+--------+-------+----------+----------------+----------+-----------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# effectuer des opérations de regroupement (group-by) sur différentes colonnes du DataFrame\n",
        "df.groupBy('Transaction_Type').count().show()\n",
        "df.groupBy('Merchant').count().show()\n",
        "df.groupBy('Location').count().show()\n",
        "df.groupBy('Entry_Mode').count().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNCAEUWyVb1p",
        "outputId": "f6cbd1f2-8bfd-4d0c-8a58-2c06ba613473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-----+\n",
            "|Transaction_Type|count|\n",
            "+----------------+-----+\n",
            "|         Deposit| 8076|\n",
            "|        Transfer| 7886|\n",
            "|         Payment| 6972|\n",
            "|      Withdrawal| 7066|\n",
            "+----------------+-----+\n",
            "\n",
            "+------------+-----+\n",
            "|    Merchant|count|\n",
            "+------------+-----+\n",
            "| Supermarket| 5575|\n",
            "| Electronics| 5726|\n",
            "|    Clothing| 6155|\n",
            "|  Restaurant| 6065|\n",
            "|Online Store| 6479|\n",
            "+------------+-----+\n",
            "\n",
            "+-----------+-----+\n",
            "|   Location|count|\n",
            "+-----------+-----+\n",
            "|    Phoenix| 5926|\n",
            "|Los Angeles| 6305|\n",
            "|    Chicago| 5520|\n",
            "|    Houston| 5305|\n",
            "|   New York| 6944|\n",
            "+-----------+-----+\n",
            "\n",
            "+---------------+-----+\n",
            "|     Entry_Mode|count|\n",
            "+---------------+-----+\n",
            "|           Chip| 7466|\n",
            "|Magnetic Stripe| 7035|\n",
            "|   Manual Entry| 7539|\n",
            "|         Online| 7960|\n",
            "+---------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only numerical columns and convert to RDD\n",
        "numerical_data = df.select(['Amount', 'User_ID','Account_Age','Previous_Transactions','Transaction_ID']).rdd.map(lambda row: row[0:])\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = Statistics.corr(numerical_data, method=\"pearson\")\n",
        "\n",
        "# Print the correlation matrix\n",
        "print(correlation_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQOOERuBXE4T",
        "outputId": "7a9b9cc3-b6b3-4ad0-a827-bddd2d94727c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.00000000e+00  4.17715755e-02 -6.20428009e-02 -8.63122712e-03\n",
            "  -6.29605324e-03]\n",
            " [ 4.17715755e-02  1.00000000e+00  9.27034915e-04  2.75727597e-02\n",
            "  -9.34355520e-03]\n",
            " [-6.20428009e-02  9.27034915e-04  1.00000000e+00 -1.50231108e-02\n",
            "   4.52745178e-02]\n",
            " [-8.63122712e-03  2.75727597e-02 -1.50231108e-02  1.00000000e+00\n",
            "   4.81353658e-02]\n",
            " [-6.29605324e-03 -9.34355520e-03  4.52745178e-02  4.81353658e-02\n",
            "   1.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Commentaire et Conclusions sur le code ci-dessus:\n",
        "\n",
        "Le code réalise une analyse de corrélation entre les colonnes numériques spécifiques du notre DataFrame en utilisant PySpark,en utlisant La corrélation de Pearson qui est un indicateur statistique qui mesure la relation linéaire entre deux variables continues, et dans ce contexte, elle est utilisée pour évaluer les associations entre les différentes colonnes numériques sélectionnées.\n",
        "\n",
        "Et d'après la sortie de ce code il s'apparait les correlations sont faible entre les varaiable assignées, donc on va appter à utliser des modèle de classifications de la bibliothèque MLlib pour prédire la fraude dans les transactions financières.\n",
        "\n"
      ],
      "metadata": {
        "id": "XXJpi_3aedSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Preprocessing Feature selection:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Le prétraitement des données et la sélection des caractéristiques (feature selection) sont deux étapes cruciales dans notre projet afin de construire des modèles performant concernant la détection de fraude dans les transactions financières.\n",
        "\n",
        "Ci-dessous on a effectuer préprocesser le jeu de donnée pour qu'il soit adaptable à notre objectif et par la suite séléctionner les caractéristiques qui vont nous permettre de construire trois modèle de detection de fraude:\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TOpgzSonZ_qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modifier la colonne 'Date' pour ne contenir que la partie date.\n",
        "df = df.withColumn(\"Date\", to_date(col(\"Date\")))\n",
        "# Ajouter une nouvelle colonne 'Time' en extrayant la partie temporelle au format HH:mm:ss.\n",
        "df = df.withColumn(\"Time\", date_format(col(\"Date\"), \"HH:mm:ss\"))\n"
      ],
      "metadata": {
        "id": "nShuO5ftai42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Afficher les colonnes Date et time après la modification effctuer sur le la colonne initiale Date.\n",
        "df.select(\"Date\", \"Time\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuTSvD4fa75W",
        "outputId": "a263cdf6-8973-4ef3-bc88-a08b0d0935b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+\n",
            "|      Date|    Time|\n",
            "+----------+--------+\n",
            "|2023-11-27|00:00:00|\n",
            "|2023-09-28|00:00:00|\n",
            "|2023-08-16|00:00:00|\n",
            "|2023-04-07|00:00:00|\n",
            "|2023-03-20|00:00:00|\n",
            "+----------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* Définir une UDF qui va nous permettre  de définir nos propres fonctions pour traiter les données d'une manière spécifique à notre besoins.Dans ce cas c'est pour générer des heures aléatoires pour résoudre le problème de temps encontourer après l'ajout de la nouvelle colonne Time.\n",
        "\n"
      ],
      "metadata": {
        "id": "WVc4gvBFb0_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_time():\n",
        "    hour = random.randint(0, 23)\n",
        "    minute = random.randint(0, 59)\n",
        "    second = random.randint(0, 59)\n",
        "    return f\"{hour:02d}:{minute:02d}:{second:02d}\"\n",
        "\n",
        "# Enregistrer la UDF (User-defined functions).\n",
        "random_time_udf = udf(random_time, StringType())"
      ],
      "metadata": {
        "id": "hJOCZyUSbo41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Afficher la colonne time après la definition de notre nouvelle fonction qui génère le temps aléatoirement.\n",
        "df = df.withColumn(\"Time\", random_time_udf())\n",
        "df.select(\"Time\").show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKY5WbtecDfx",
        "outputId": "0dce0e40-ffb9-4f5b-cf77-962b1f3a690e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|    Time|\n",
            "+--------+\n",
            "|20:42:26|\n",
            "|02:56:06|\n",
            "|16:31:38|\n",
            "|09:34:00|\n",
            "|11:03:05|\n",
            "+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extraire l'heure à partir de la colonne 'Time' .\n",
        "df = df.withColumn(\"Hour\", hour(df[\"Time\"]))\n",
        "#Extraire (Year,Month,Day) à partire de la colonne \"Date\" en mettant chachune dans une nouvelle colonne.\n",
        "df = df.withColumn(\"Year\", year(col(\"Date\")))\n",
        "df = df.withColumn(\"Month\", month(col(\"Date\")))\n",
        "df = df.withColumn(\"Day\", dayofweek(col(\"Date\")))\n",
        "#ajouter une nouvelle colonne \"HourOfDay\" au DataFrame df.\n",
        "df = df.withColumn(\"HourOfDay\", hour(col(\"Time\")))\n"
      ],
      "metadata": {
        "id": "qdnxGRuUfrd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# génère une liste d'indexeurs de chaînes pour les colonnes spécifiées dans le DataFrame. Ces indexeurs seront utilisés pour transformer les valeurs catégoriques de ces colonnes en indices numériques.\n",
        "#Chaque indexeur créé sera utilisé ultérieurement pour appliquer cette transformation aux données réelles.\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=column, outputCol=column + \"_Indexed\").fit(df)\n",
        "    for column in [\"Transaction_Type\", \"Location\", \"Merchant\"]\n",
        "]\n"
      ],
      "metadata": {
        "id": "xhD4V3Gwizsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-Ce code Ci-dessus assemble les caractéristiques numériques et catégoriques, puis les met à l'échelle pour préparer les données en vue d'une utilisation du modèle d'apprentissage automatique.Le processus d'assemblage est nécessaire car de nombreux algorithmes d'apprentissage automatique s'attendent à recevoir toutes les caractéristiques dans un seul vecteur,tandis que la mise à l'échelle est souvent nécessaire pour normaliser les caractéristiques numériques."
      ],
      "metadata": {
        "id": "I0KC7sewrbC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = [\"Location\", \"Merchant\", \"Transaction_Type\"]\n",
        "#'features' inclura toutes les colonnes de caractéristiques après le prétraitement.\n",
        "assembler = VectorAssembler(inputCols=[\"Amount\", \"Year\", \"Month\", 'Day', \"HourOfDay\"] + [c + \"_Index\" for c in categorical_columns], outputCol=\"assembled_features\")\n",
        "\n",
        "# Scale features pour  la mise à l'échelle des caractéristiques (features) dans l'ensemble de données.\n",
        "scaler = StandardScaler(inputCol=\"assembled_features\", outputCol=\"scaledFeatures\")\n"
      ],
      "metadata": {
        "id": "GxXjHwcJzjnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir'Fraudulent'to double type for the classifier\n",
        "#prépare la variable cible  en s'assurant qu'elle est du type de données approprié, ici, DoubleType().\n",
        "df = df.withColumn(\"label\", col(\"Fraudulent\").cast(DoubleType()))"
      ],
      "metadata": {
        "id": "0WOAprNTjgYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.construction du modèle:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Après le préprocessing du jeu de données et la séléction de ses différents caractéristiques, dans cette étape on va construire notre modèle en suivant les le processus suivant:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> Choix de modèle.\n",
        "\n",
        "> Division du jeu de donnée en un ensemble de test et d'entrainnement.\n",
        "\n",
        "> Entrainement du modèle.\n",
        "\n",
        "> Evaluation du modèle\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sbz4zHUS6Hh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importations des bibliothèques nécessaires:\n",
        "from pyspark.ml.classification import GBTClassifier, DecisionTreeClassifier\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf, col, to_date, dayofweek, hour\n",
        "from pyspark.sql.types import StringType, DoubleType\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ],
      "metadata": {
        "id": "Hakkn9EcfRVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifier\n",
        "rf = RandomForestClassifier(featuresCol=\"scaledFeatures\", labelCol=\"label\")\n",
        "# Pipeline\n",
        "pipeline = Pipeline(stages=indexers + [assembler, scaler, rf])"
      ],
      "metadata": {
        "id": "ucqDudMLPLyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Diviser le jeu de donnée en un ensemble d'entrainnement et de tests:\n",
        "train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)"
      ],
      "metadata": {
        "id": "pS094Uh_mbzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Le code ci-dessous  en PySpark utilise trois modèles différents de classification d'arbres pour entraîner des classificateurs et évaluer leurs performances sur L'ensemble de données.\n",
        "\n"
      ],
      "metadata": {
        "id": "nQ9NzLCs7vPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import GBTClassifier, DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Gradient-Boosted Tree Classifier\n",
        "gbt = GBTClassifier(featuresCol=\"scaledFeatures\", labelCol=\"label\", maxIter=10)\n",
        "\n",
        "# Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(featuresCol=\"scaledFeatures\", labelCol=\"label\")\n",
        "# Pipeline for Random Forest\n",
        "rf_pipeline = Pipeline(stages=indexers + [assembler, scaler, rf])\n",
        "\n",
        "# Pipeline pour Gradient-Boosted Trees\n",
        "gbt_pipeline = Pipeline(stages=indexers + [assembler, scaler, gbt])\n",
        "\n",
        "# Pipeline pour Decision Tree\n",
        "dt_pipeline = Pipeline(stages=indexers + [assembler, scaler, dt])\n",
        "# List of model pipelines\n",
        "model_pipelines = [(\"Random Forest\", rf_pipeline),\n",
        "                   (\"Gradient-Boosted Trees\", gbt_pipeline),\n",
        "                   (\"Decision Tree\", dt_pipeline)]\n",
        "\n",
        "for model_name, pipeline in model_pipelines:\n",
        "    # Fit the model\n",
        "    fitted_model = pipeline.fit(train_data)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    predictions = fitted_model.transform(test_data)\n",
        "\n",
        "    # Evaluate the model using AUC as the metric\n",
        "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
        "    auc = evaluator.evaluate(predictions)\n",
        "\n",
        "    # Print the evaluation results\n",
        "    print(f\"{model_name} AUC: {auc}\")\n"
      ],
      "metadata": {
        "id": "sz6y3noM2rxS",
        "outputId": "b872423d-5f2b-46f9-d48c-7d32f883bf09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest AUC: 0.8208788511361574\n",
            "Gradient-Boosted Trees AUC: 0.9487648311028234\n",
            "Decision Tree AUC: 0.40607477021072946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Ce code ci- dessous en PySpark évalue la performance de modèles de classification en utilisant différents métriques d'évaluation, tels que l'AUC (Area Under the ROC Curve), la précision, le rappel et le score F1\n",
        "\n"
      ],
      "metadata": {
        "id": "Jgc5xxYa9AkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "for model_name, pipeline in model_pipelines:\n",
        "    # Fit the model\n",
        "    fitted_model = pipeline.fit(train_data)\n",
        "\n",
        "    # faire des predictions sur l'ensemble de test\n",
        "    predictions = fitted_model.transform(test_data)\n",
        "\n",
        "    # Evaluer le modele en utlisant AUC\n",
        "\n",
        "    auc = binary_evaluator.evaluate(predictions, {binary_evaluator.metricName: \"areaUnderROC\"})\n",
        "\n",
        "    # Evaluer le  modele en utlisant Precision\n",
        "    precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"precisionByLabel\"})\n",
        "\n",
        "    # Evaluer le modele en utilisant Recall\n",
        "    recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"recallByLabel\"})\n",
        "\n",
        "    # Evaluer le modele en utlisant F1 Score\n",
        "    f1_score = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})\n",
        "\n",
        "    # Print the evaluation results\n",
        "    print(f\"{model_name} Results:\")\n",
        "    print(f\"  AUC: {auc}\")\n",
        "    print(f\"  Precision: {precision}\")\n",
        "    print(f\"  Recall: {recall}\")\n",
        "    print(f\"  F1 Score: {f1_score}\\n\")\n"
      ],
      "metadata": {
        "id": "BQzsGju03cUn",
        "outputId": "609db312-4fdd-4623-bbc4-19a2c1c4cfae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Results:\n",
            "  AUC: 0.8071742107955573\n",
            "  Precision: 0.9696460573476703\n",
            "  Recall: 1.0\n",
            "  F1 Score: 0.9556929191120129\n",
            "\n",
            "Gradient-Boosted Trees Results:\n",
            "  AUC: 0.9487648311028234\n",
            "  Precision: 0.9711689477226835\n",
            "  Recall: 1.0\n",
            "  F1 Score: 0.9592578874069271\n",
            "\n",
            "Decision Tree Results:\n",
            "  AUC: 0.40607477021072946\n",
            "  Precision: 0.97131931166348\n",
            "  Recall: 0.9975742173963267\n",
            "  F1 Score: 0.958239341500121\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the data for the table\n",
        "data = {\n",
        "    \"Modèle\": [\"Random Forest\", \"Gradient-Boosted Trees\", \"Decision Tree\"],\n",
        "    \"AUC\": [0.8071, 0.9487, 0.4060],\n",
        "    \"Précision\": [0.9696, 0.9711, 0.9713],\n",
        "    \"Rappel\": [1.0, 1.0, 0.9975],\n",
        "    \"F1 Score\": [0.9556, 0.9952, 0.9582]\n",
        "}\n",
        "\n",
        "# Create the DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Print the DataFrame to display the table\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "ChNPqnbgAj4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MDdCX8PeBAJr"
      }
    }
  ]
}